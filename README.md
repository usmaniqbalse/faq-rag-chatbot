# Fully Offline RAG Question & Answering System

A minimal Retrieval-Augmented Generation (RAG) app using **Streamlit** (UI), **ChromaDB** (vector store), **Ollama** (LLM + embeddings), and a **CrossEncoder** re-ranker.  
Upload a PDF ‚Üí it‚Äôs chunked & embedded ‚Üí you ask a question ‚Üí we retrieve + re-rank ‚Üí the LLM answers **only from the retrieved context**.

> ‚ö†Ô∏è **‚ÄúFully offline‚Äù note:** the CrossEncoder model is downloaded once on first run. After it‚Äôs cached locally, you can run offline.

---

## Table of Contents

1. [What You‚Äôll Build](#what-youll-build)
2. [Prerequisites](#prerequisites)
3. [Quick Start (5 minutes)](#quick-start-5-minutes)
4. [Project Structure](#project-structure)
5. [How It Works](#how-it-works)
6. [Troubleshooting](#troubleshooting)
7. [FAQ](#faq)
8. [Credits](#credits)

---

## What You‚Äôll Build

- **Document processing:** PDF ‚Üí text ‚Üí chunks (size **400**, overlap **100**).
- **Embeddings & Vector Store:** `nomic-embed-text:latest` via **Ollama** ‚Üí stored in **ChromaDB** (`./demo-rag-chroma`).
- **Retrieval:** Top-k semantic matches from Chroma.
- **Re-ranking:** CrossEncoder `cross-encoder/ms-marco-MiniLM-L-6-v2` picks the **top 3** most relevant chunks.
- **Answering:** `llama3.2:3b` via **Ollama** with a strict system prompt to ground answers in context only.

---

## Prerequisites

- **Python** 3.9‚Äì3.11
- **Ollama** installed locally and listening on `http://localhost:11434`
- **Git**
- **One-time internet** to download the CrossEncoder (after that: offline OK)

> GPU optional. Ollama falls back to CPU if needed (slower).

---

## Quick Start (5 minutes)

1. **Clone & enter the repo**

   ```bash
   git clone https://github.com/usmaniqbalse/faq-rag-chatbot.git
   cd faq-rag-chatbot
   ```

2. **Create and activate a virtual environment**

   ```bash
   # macOS/Linux
   python3 -m venv .venv
   source .venv/bin/activate

   # Windows PowerShell
   python -m venv .venv
   .venv\Scripts\Activate.ps1
   ```

3. **Install Ollama & pull models**

   ```bash
   # Install Ollama (see ollama.com for OS-specific steps)
   # macOS (Homebrew):
   #   brew install ollama
   # Linux:
   #   curl -fsSL https://ollama.com/install.sh | sh
   # Windows:
   #   Use the official installer

   # Start/ensure Ollama is running:
   ollama serve  # (may be a no-op if it's already running)

   # Pull the models used by this app:
   ollama pull llama3.2:3b
   ollama pull nomic-embed-text:latest
   ```

   **Verify Ollama API**

   ```bash
   curl http://localhost:11434/api/tags
   # Should return JSON listing local models
   ```

4. **Install Python dependencies**

   Ensure your `requirements.txt` contains (or install directly):

   ```txt
   streamlit
   chromadb
   ollama
   sentence-transformers
   pymupdf
   langchain-core
   langchain-community
   langchain-text-splitters
   ```

   Install:

   ```bash
   pip install -r requirements.txt
   # or:
   pip install streamlit chromadb ollama sentence-transformers pymupdf \
               langchain-core langchain-community langchain-text-splitters
   ```

5. **(Optional) Create the Chroma data directory**

   Chroma will create it if missing, but you can do it explicitly:

   ```bash
   mkdir -p demo-rag-chroma
   ```

6. **Run the app**

   ```bash
   streamlit run main.py
   ```

   Open: [http://localhost:8501](http://localhost:8501)

   **Use the app**

   - In the sidebar, upload a PDF and click ‚ö°Ô∏è Process (embeds & stores chunks).
   - In the main area, type your question and click üî• Ask.
   - Read the streamed answer. Expand the panels to see retrieved docs and the selected most-relevant chunks.

---

## Project Structure

```
.
‚îú‚îÄ‚îÄ main.py                       # Streamlit entrypoint (UI + orchestration)
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ system_prompt.py          # System prompt used by the LLM
‚îî‚îÄ‚îÄ utilities/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ document_processing.py    # PDF -> text -> chunks
    ‚îú‚îÄ‚îÄ model_inference.py        # Ollama chat call (llama3.2:3b)
    ‚îî‚îÄ‚îÄ vector_store.py           # Chroma collection + upsert + query

# Runtime data:
demo-rag-chroma/                  # Chroma persistent store (auto-created)
```

> If you previously had a single `app.py`, this project uses **`main.py`** as the entry.

---

## How It Works

1. **Upload & Ingest**  
   The PDF is written to a temp file, parsed with **PyMuPDF**, and split using `RecursiveCharacterTextSplitter` (size=400, overlap=100).  
   Embeddings are generated by **Ollama** with `nomic-embed-text:latest`.  
   Chunks + metadata are **upserted** into a persistent **ChromaDB** collection at `./demo-rag-chroma`.

2. **Ask a Question**  
   The app queries Chroma (`n_results=10`) to get the most similar chunks.

3. **Re-rank**  
   CrossEncoder `cross-encoder/ms-marco-MiniLM-L-6-v2` ranks those chunks and we keep the **top 3** (concatenated as context).

4. **Generate Answer (Grounded)**  
   **Ollama** runs `llama3.2:3b` with a strict **SYSTEM_PROMPT** to answer **only** from the provided context.  
   The response is streamed to the UI.

5. **Inspect**  
   Expand ‚ÄúSee retrieved documents‚Äù and ‚ÄúSee most relevant document ids‚Äù to debug the pipeline.

---

## Troubleshooting

**`ConnectionError: Failed to connect to Ollama`**

- Ensure the service is running: `ollama serve`
- Health check: `curl http://localhost:11434/api/tags`
- Port conflict? Make sure nothing else uses `11434`.

**CrossEncoder can‚Äôt download (no internet on first run)**

Run once with internet so `sentence-transformers` caches the model:

```python
from sentence_transformers import CrossEncoder
CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
```

After caching (typically under `~/.cache/torch` or `~/.cache/huggingface`), you can go fully offline.

**‚ÄúNo results found‚Äù or weak answers**

- Click ‚ö°Ô∏è Process after uploading the PDF.
- Ask questions covered by the PDF content.
- Consider adding more documents or (with code changes) adjusting chunk size/overlap.

**Reset the vector store**

```bash
rm -rf demo-rag-chroma
mkdir demo-rag-chroma
```

Re-process your PDFs.

**Change models or storage path**

- Embedding model & Chroma path are in `utilities/vector_store.py`.
- Chat model & system prompt are in `utilities/model_inference.py` and `prompts/system_prompt.py`.

---

## FAQ

**Is it really ‚Äúfully offline‚Äù?**  
After pulling Ollama models and the first CrossEncoder download/cache, yes.

**Why only PDFs?**  
We use `PyMuPDFLoader`. Supporting DOCX/HTML/etc. requires code changes.

**Where are embeddings stored?**  
In `./demo-rag-chroma` (configurable in `vector_store.py`).

**How many chunks are used to answer?**  
Retrieve up to 10, then re-rank and keep the **top 3**.

---

## Credits

- [ChromaDB](https://chromadb.com/) ‚Äî vector database
- [Ollama](https://ollama.com/) ‚Äî local LLMs & embeddings
- [Streamlit](https://streamlit.io/) ‚Äî rapid UI
- [Sentence-Transformers](https://www.sbert.net/) ‚Äî CrossEncoder re-ranking

---

Happy building! Run `streamlit run main.py`, upload a PDF, and start asking grounded questions. ‚ú®
